{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-28T17:42:11.219323Z",
     "start_time": "2025-04-28T17:42:09.685285Z"
    }
   },
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "from datasets import load_dataset\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"] = \"qwen-coder-llm-fine-tuning\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:42:13.518041Z",
     "start_time": "2025-04-28T17:42:11.420147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "wandb.login()"
   ],
   "id": "3a68ea633eb542c9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: casvi-sanchez (virtualtek) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:42:13.540045Z",
     "start_time": "2025-04-28T17:42:13.535733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if wandb.run is not None:\n",
    "  wandb.finish()"
   ],
   "id": "31e51434f41a8252",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model_id = \"Qwen/Qwen2.5-Coder-0.5B-Instruct\"",
   "id": "719a98d137be9d82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:42:21.433049Z",
     "start_time": "2025-04-28T17:42:13.555263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_name = \"squad_v2\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "eval_dataset = load_dataset(dataset_name, split=\"validation\")\n",
    "print(\"dataset: \",dataset)\n",
    "print(\"eval_dataset: \",eval_dataset)\n"
   ],
   "id": "619694e96549ecc6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:  Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 130319\n",
      "})\n",
      "eval_dataset:  Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 11873\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:42:25.146940Z",
     "start_time": "2025-04-28T17:42:21.476877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "if cuda_available:\n",
    "    device_id = 0  # You can change to 1,2,3 if you want other GPUs\n",
    "    torch.cuda.set_device(device_id)\n",
    "    # device = torch.device(f\"cuda:{device_id}\")\n",
    "    device = torch.device(f\"cuda:{device_id}\")\n",
    "    print(f\"üñ•Ô∏è Using GPU {device_id}: {torch.cuda.get_device_name(device_id)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"‚öôÔ∏è No GPU available, using CPU.\")\n",
    "\n",
    "print(f\"Device selected: {device}\")"
   ],
   "id": "8c5e9509b1624cc7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è Using GPU 0: NVIDIA GeForce RTX 4070 SUPER\n",
      "Device selected: cuda:0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:42:28.439325Z",
     "start_time": "2025-04-28T17:42:25.163605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "device_map=\"auto\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "print(base_model.device)  # Shows cuda:0 or cpu"
   ],
   "id": "683ca904b1ac93b5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "C:\\Python\\PyProjects\\Code-Fixer-LLM-Agent\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:807: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  _ = torch.tensor([0], device=i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:42:28.457643Z",
     "start_time": "2025-04-28T17:42:28.453644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_dir = \"./results\"\n",
    "login_dir=\"./logs\""
   ],
   "id": "7bf2ef284fb4d400",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:42:29.103935Z",
     "start_time": "2025-04-28T17:42:28.492943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, EarlyStoppingCallback\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# More info: https://github.com/huggingface/transformers/pull/24906\n",
    "base_model.config.pretraining_tp = 1\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# if you don't want to use google drive, just set this to a directory on your computer:\n",
    "new_model_name = f\"qwen-3b-peft-{dataset_name}\""
   ],
   "id": "97d4e225a5fb044",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:42:29.909157Z",
     "start_time": "2025-04-28T17:42:29.136867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ],
   "id": "7e8c296f04df3705",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T17:42:29.949069Z",
     "start_time": "2025-04-28T17:42:29.943893Z"
    }
   },
   "cell_type": "code",
   "source": "optim=torch.optim.Adam(base_model.parameters(), lr=1e-5)",
   "id": "50f7b6c575745e2d",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:24:07.586604Z",
     "start_time": "2025-04-28T17:42:29.988169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    logging_dir=login_dir,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    max_steps=500,\n",
    "    num_train_epochs=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_total_limit=5,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    dataset_text_field=\"question\",\n",
    "    max_seq_length=512,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "# Initialize the trainer with the configuration\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    args=training_args,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=200)],\n",
    "    optimizers=(optim,None),\n",
    "    compute_metrics=None\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "end = time.time()\n",
    "length = end - start\n",
    "\n",
    "hours = int(length // 3600)\n",
    "minutes = int((length % 3600) // 60)\n",
    "seconds = int(length % 60)\n",
    "\n",
    "print(f\"It took {hours} hours, {minutes} minutes, and {seconds} seconds to train the model!\")"
   ],
   "id": "3a9e234f9165bc69",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Python\\PyProjects\\Code-Fixer-LLM-Agent\\wandb\\run-20250428_114231-27dw86lz</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/virtualtek/qwen-coder-llm-fine-tuning/runs/27dw86lz' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/virtualtek/qwen-coder-llm-fine-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/virtualtek/qwen-coder-llm-fine-tuning' target=\"_blank\">https://wandb.ai/virtualtek/qwen-coder-llm-fine-tuning</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/virtualtek/qwen-coder-llm-fine-tuning/runs/27dw86lz' target=\"_blank\">https://wandb.ai/virtualtek/qwen-coder-llm-fine-tuning/runs/27dw86lz</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 41:32, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.844900</td>\n",
       "      <td>5.373620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.878100</td>\n",
       "      <td>5.373620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.835200</td>\n",
       "      <td>5.373620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.838800</td>\n",
       "      <td>5.373620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.908600</td>\n",
       "      <td>5.373620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0 hours, 41 minutes, and 37 seconds to train the model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:39:21.152714Z",
     "start_time": "2025-04-28T18:37:08.106695Z"
    }
   },
   "cell_type": "code",
   "source": "print(trainer.evaluate())",
   "id": "e5c7d5f443b51dca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.37362003326416, 'eval_runtime': 133.0385, 'eval_samples_per_second': 89.245, 'eval_steps_per_second': 11.162}\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:24:08.096460Z",
     "start_time": "2025-04-28T16:26:08.255249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer.model.save_pretrained(\n",
    "    os.path.join(output_dir, \"final_checkpoint\"),\n",
    " )"
   ],
   "id": "e8500b862c041e69",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:24:08.096977Z",
     "start_time": "2025-04-28T16:39:30.979363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": torch.cuda.current_device()},\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Step 2: Load your fine-tuned LoRA adapter\n",
    "adapter_path = os.path.join(output_dir, \"final_checkpoint\")\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "# Step 3: Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Step 4: Generate predictions\n",
    "prompt = \"Question: What areas did Beyonce compete in when she was growing up??\\nAnswer:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ],
   "id": "262ea5decd125eed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What areas did Beyonce compete in when she was growing up??\n",
      "Answer: Beyonce competed in several different areas when she was growing up, including:\n",
      "\n",
      "1. Advertising: Beyonce was very active in advertising and used various platforms to promote her work.\n",
      "\n",
      "2. Music: She worked on various music projects, including the song \"Beyonc√©\" and \"My Beautiful Boy\".\n",
      "\n",
      "3. Sports: Beyonce played a significant part in sports, including her skating career and her boxing career.\n",
      "\n",
      "4. Art: She worked on various art projects, including the painting \"Beyonc√©\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "95a448f21ce76437"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
